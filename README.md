# ğŸˆâ€â¬› FURIApp Connect

**FURIApp Connect** Ã© um aplicativo web criado para o desafio tÃ©cnico da FURIA Tech. Seu objetivo Ã© conectar fÃ£s da equipe FURIA atravÃ©s de perfis personalizados e interaÃ§Ã£o inteligente com uma IA que entende o universo FURIA.

## ğŸš€ DemonstraÃ§Ã£o

[ğŸ”— Link para o vÃ­deo demonstrativo](#)  
[ğŸ”— Link para o deploy](https://furiapp-connect.vercel.app/)

## ğŸ¯ Funcionalidades

- **Landing Page**: ApresentaÃ§Ã£o do projeto com identidade visual da FURIA.
- **FormulÃ¡rio do FÃ£**: Coleta de informaÃ§Ãµes como jogador favorito, mapa preferido e estilo de torcedor.
- **GeraÃ§Ã£o de Perfil**: Perfil exclusivo gerado com base nas respostas do fÃ£.
- **Match entre FÃ£s**: SugestÃµes de fÃ£s com perfis semelhantes.
- **IA FURIApp**: Chat inteligente alimentado por um modelo local da plataforma [Ollama](https://ollama.com/), que conversa com o fÃ£ usando contexto do seu perfil.
- **PÃ¡gina "Sobre"**: InformaÃ§Ãµes sobre a proposta do projeto, stack utilizada e equipe (vocÃª!).

## ğŸ¤– IntegraÃ§Ã£o com IA

A aplicaÃ§Ã£o inclui um chatbot local baseado em modelos LLM servidos via **Ollama**, permitindo:

- Conversas contextuais personalizadas para cada fÃ£.
- RecomendaÃ§Ãµes de conteÃºdos e curiosidades sobre a FURIA.
- GeraÃ§Ã£o de respostas sem depender de APIs externas, garantindo privacidade e performance.

> Para rodar a IA localmente, Ã© necessÃ¡rio ter o Ollama instalado na mÃ¡quina.

## ğŸ› ï¸ Tecnologias Utilizadas

- [Next.js](https://nextjs.org/)
- [React](https://reactjs.org/)
- [TypeScript](https://www.typescriptlang.org/)
- [Tailwind CSS](https://tailwindcss.com/)
- [Ollama](https://ollama.com/) â€“ para LLM local

## ğŸ“ Estrutura de Pastas

```
â”œâ”€â”€ app/
â”œâ”€â”€ components/
â”œâ”€â”€ lib/
â”œâ”€â”€ styles/
â”œâ”€â”€ public/
â”œâ”€â”€ .vscode/
â”œâ”€â”€ package.json
â”œâ”€â”€ tailwind.config.ts
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## ğŸ”§ Como Rodar Localmente

1. **Clone o repositÃ³rio**:

   ```bash
   git clone https://github.com/nathartigas/FURIApp_Connect.git
   ```

2. **Instale as dependÃªncias**:

   ```bash
   npm install
   ```

3. **Inicie o backend da IA (Ollama)**:

   - Instale o Ollama: [https://ollama.com/download](https://ollama.com/download)
   - Rode o modelo local desejado:

     ```bash
     ollama run mistral
     ```

   *(ou outro modelo compatÃ­vel com o projeto)*

4. **Inicie o app**:

   ```bash
   npm run dev
   ```

5. **Acesse no navegador**:

   [http://localhost:3000](http://localhost:3000)

- **Nathalia Artigas**  
  [LinkedIn]([https://www.linkedin.com/in/nathalia-artigas/](https://www.linkedin.com/in/nathalia-calazans-artigas-741b0b277/)) | [GitHub](https://github.com/nathartigas)
